---
layout:     post
title:      浅谈机器学习中的文本摘要
subtitle:   A Gentle Introduction to Text Summarization in Machine Learning
date:       2019-05-21
author:     YunLambert
header-img: img/post-bg-ML_TextSummarization.png
catalog: true
tags:
    - Review
---

> A Gentle Introduction to Text Summarization in Machine Learning

原文自[Here](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)

摘要是一种在集中于传达有用信息的章节的同时，在不失去整体意义的情况下，对大量的文本进行简洁而精确的摘要的技术。将长文本压缩成摘要形式是冗长且复杂的，可以借助机器学习算法，训练、理解文档并识别传达重要事实和信息的部分。

概括地说，在自然语言处理中总结文本有两种方法：提取和抽取。
## 提取和抽取

### 提取

在机器学习中，提取总结通常涉及到权衡句子的基本部分，并使用结果生成总结。可以使用不同类型的算法和方法来衡量句子的权重，然后根据它们之间的相关性和相似性对它们进行排序-并进一步加入它们来生成一个摘要。

### 抽取

抽象机器学习算法可以生成新的短语和句子，这些短语和句子代表了源文本中最重要的信息，有助于克服提取技术的语法不准确之处。

尽管抽象在文本摘要方面表现得更好，但是开发它的算法需要复杂的深入学习技术和复杂的语言建模。为了生成合理的输出，基于抽象的摘要方法必须解决各种各样的自然语言处理问题，如自然语言生成、语义表示和推理置换。因此，抽取文本摘要的方法仍然很受欢迎。在本文中，我们将重点介绍一种基于提取的方法。

## 基于提取的方法

下面是要被提取的原文本：

> *“Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital. Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. Therefore, Peter stayed with her at the hospital for 3 days without leaving.”*

### 步骤一：将段落转换为句子

1. Peter and Elizabeth took a taxi to attend the night party in the city

2. While in the party, Elizabeth collapsed and was rushed to the hospital

3. Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well

4. Therefore, Peter stayed with her at the hospital for 3 days without leaving

### 步骤二：文本处理

除去句子中的“停顿词”（“and”和“the”等极常见的几乎没有意义的词）。执行过滤有助于删除多余和不重要的信息，这些信息可能不会为文本的意义提供任何附加价值。

1. Peter Elizabeth took taxi attend night party city

2. Party Elizabeth collapse rush hospital

3. Diagnose brain injury doctor told Peter stay besides get well

4. Peter stay hospital days without leaving

### 步骤三：词语切分

```
['peter','elizabeth','took','taxi','attend','night','party','city','party','elizabeth','collapse','rush','hospital', 'diagnose','brain', 'injury', 'doctor','told','peter','stay','besides','get','well','peter', 'stayed','hospital','days','without','leaving']
```

### 步骤四：计算字词的加权出现频率

| WORD      | FREQUENCY | WEIGHTED FREQUENCY |
| --------- | --------- | ------------------ |
| peter     | 3         | 1                  |
| elizabeth | 2         | 0.67               |
| took      | 1         | 0.33               |
| taxi      | 1         | 0.33               |
| attend    | 1         | 0.33               |
| night     | 1         | 0.33               |
| party     | 2         | 0.67               |
| city      | 1         | 0.33               |
| collapse  | 1         | 0.33               |
| rush      | 1         | 0.33               |
| hospital  | 2         | 0.67               |
| diagnose  | 1         | 0.33               |
| brain     | 1         | 0.33               |
| injury    | 1         | 0.33               |
| doctor    | 1         | 0.33               |
| told      | 1         | 0.33               |
| stay      | 2         | 0.67               |
| besides   | 1         | 0.33               |
| get       | 1         | 0.33               |
| well      | 1         | 0.33               |
| days      | 1         | 0.33               |
| without   | 1         | 0.33               |
| leaving   | 1         | 0.33               |

### 步骤五：用加权频率替换单词

替换以后，计算每个句子出现频率单词的和。

| SENTENCE | ADD WEIGHTED FREQUENCIES                                     | SUM                                                          |      |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| 1        | Peter and Elizabeth took a taxi to attend the night party in the city | 1 + 0.67 + 0.33 + 0.33 + 0.33 + 0.33 + 0.67 + 0.33           | 3.99 |
| 2        | While in the party, Elizabeth collapsed and was rushed to the hospital | 0.67 + 0.67 + 0.33 + 0.33 + 0.67                             | 2.67 |
| 3        | Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. | 0.33 + 0.33 + 0.33 + 0.33 + 1 + 0.33 + 0.33 + 0.33 + 0.33 +0.33 | 3.97 |
| 4        | Therefore, Peter stayed with her at the hospital for 3 days without leaving | 1 + 0.67 + 0.67 + 0.33 + 0.33 + 0.33                         | 3.33 |

可以计算出第一个句子在整段中具有较高的权重，因此可以最佳的概括整个段落。此外，如果第一句与第三句结合起来，就可以产生更好的摘要。上面的例子只是给出了如何在机器学习中执行基于提取的文本摘要的基本说明。现在，让我们看看如何将上面的概念进行实现。

## 提取方法的实现

Yun：文本资料我自己找了一篇[文章](https://arp242.net/json-config.html)，而且原文中的文本需要上外网，并且爬虫做的不太完善，所以这里借鉴原文写了以下程序。以及自己找的文章如果让我总结一下："Using JSON for configuration files is not a good idea."

放上程序：

```python
import requests
from bs4 import BeautifulSoup as bs4
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer


def get_article_text(url):
    data = requests.get(url)
    if data.status_code != 200:
        return "error while crawling..."

    parsed_data = bs4(data.text, "html.parser")
    paragraphs = parsed_data.find_all('p')

    paragraphs_text = ''
    for p in paragraphs:
        paragraphs_text += p.text

    return paragraphs_text


def change_into_sentences(paragraphs_text):
    sentences_text = sent_tokenize(paragraphs_text)
    return sentences_text


def create_frequency_table(article_text):
    stop_words = set(stopwords.words("english"))  # 设定停顿词
    words = word_tokenize(article_text)  # 单词切分
    stem = PorterStemmer()  # 将单词还原为词根形式

    # 创建单词频度表
    frequency_table = dict()
    for word in words:
        word = stem.stem(word)
        if word in stop_words:
            continue
        if word in frequency_table:
            frequency_table[word] += 1
        else:
            frequency_table[word] = 1

    return frequency_table


def cal_sentence_scores(sentences_text, frequency_table):
    sentence_weight = dict()
    for sentence in sentences_text:
        sentence_wordcount = len(word_tokenize(sentence))
        sentence_wordcount_without_stop_words = 0
        for word_weight in frequency_table:
            if word_weight in sentence.lower():
                sentence_wordcount_without_stop_words += 1
                if sentence[:7] in sentence_weight:
                    sentence_weight[sentence[:7]] += frequency_table[word_weight]
                else:
                    sentence_weight[sentence[:7]] = frequency_table[word_weight]

        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words

    return sentence_weight


def cal_avg_score(sentence_weight):
    sum_values = 0
    for entry in sentence_weight:
        sum_values += sentence_weight[entry]
    average_score = (sum_values / len(sentence_weight))

    return average_score


def get_summary_text(article):
    frequency_table = create_frequency_table(article)
    sentences = change_into_sentences(article)
    sentence_scores = cal_sentence_scores(sentences, frequency_table)
    threshold = 1.5 * cal_avg_score(sentence_scores)

    sentence_counter = 0
    article_summary = ''

    for sentence in sentences:
        if sentence[:7] in sentence_scores and sentence_scores[sentence[:7]] >= threshold:
            article_summary += " " + sentence
            sentence_counter += 1

    return article_summary


if __name__ == "__main__":
    url = 'https://arp242.net/json-config.html'
    paragraphs_text = get_article_text(url)
    summary_results = get_summary_text(paragraphs_text)
    print(summary_results)

```

结果出来的是这个：

```
I think this is not a good idea.It’s not what JSON was designed to do, and consequently not what it’s good at. The occasion
al JSON parser supports it, but most don’t and it’s not in the standard. These properties are good for JSON’s intended usage, but not so great for editing configuration files. Of course. Declarative configuration (DC)
works well for some problems, but not so well for others. After all, that’s what code is for.A lot of people have asked me for suggestions about what to use. All of these seem like a reasonable step up from regular JSON, although I have not used any of them myself.
```

效果有点。。。差。。。但是大致思想就是这样的。

## 后续操作

因为本文中的实现方式十分简单，所以更换停顿词数据集、计算单词相似度、神经网络训练等等都可以提升性能和精准度。
