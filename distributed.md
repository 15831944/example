# 分布式系统

## Zookeeper 分布式服务框架
## etcd
## Consul
## Doozerd
## Gearman 分布式任务处理系统
## Celery 分布式任务队列
## NSQ 分布式的实时消息平台
## Apache Kafka
## Disque
## RabbitMQ
## Apache ActiveMQ
## ZeroMQ
## Disque 分布式消息中间件
## Beanstalkd 分布式消息队列
## nanomsg
## Darner

## Druid
## PipelineDB

## Apache Storm 流式处理框架
## Apache Spark

## Apache Hadoop
## Apache Hive
## Apache HBase

## Disco - Massive data, Minimal code

## Apache Flume

## Zabbix 开源分布式监控解决方案

## ElasticSearch 分布式全文搜索引擎

## GFS、HDFS、Lustre 、Ceph 、GridFS 、MogileFS、TFS、FastDFS、MooseFS、LizardFS、GlusterFS

## Quartz、Rundeck


摘自：用大白话聊聊分布式系统 https://waylau.com/talk-about-distributed-system/

* 什么是分布式系统

关于“分布式系统”的定义，我们先看下老外是怎么说的。《分布式系统原理和范型》一书中是这样定义分布式系统的：“分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关系统”。

关于这个定义，我们直观的感受就是：

- 首先，这种系统相对来说比较牛逼，起码由好几台主机组成。以谷歌、亚马逊等服务商而言，他们的数据中心都由上万台主机支撑起来的。
- 其次，虽然很牛逼，但对于外人来说，是感觉不到这些主机的存在。也就是说，我们只看到是一个系统在运作。以最近的“亚马逊 S3 宕机事件”为例，平时，我们压根不知道亚马逊所提供的服务背后是由多少台主机组成，但是等到 S3 宕机才知道，这货已经是占了互联网世界的半壁江山了。

从进程角度看，两个程序分别运行在两个台主机的进程上，它们相互协作最终完成同一个服务（或者功能），那么理论上这两个程序所组成的系统，也可以称作是“分布式系统”。

当然，这个两个程序可以是不同的程序，也可以是相同的程序。如果是相同的程序，我们又可以称之为“集群”。所谓集群，就是将相同的程序，通过不断横向扩展，以提高服务能力的方式。

“分布式系统”和“集群”的定义够都简单吧。

* 分布式系统有哪些优势

那么，为啥我们要用分布式系统？

说起分布式系统，我们就不得不说下分布式系统的祖先——集中式系统。集中式系统跟分布式系统是完全相反的两个概念。集中式系统就是把所有的程序、功能都集中到一台主机上，从而往外提供服务的方式。

集中式系统最容易理解了。比如，我们主机的PC电脑，或者手机，我们把各种软件都安装在一台机子上，当我需要什么功能，我就从这台机子上去获取。再比如，我们在学生时代做的课程设计或者开发时的小应用，我们把Web服务器、数据库等都会安装到一台电脑上。好处是，易于理解、方便维护，想要的东西我都放到了一个地方，东西好找啊。当然弊端也是显而易见的，如果这台机子崩了，或者硬盘坏了，那相当与整个系统就奔溃了，而且如果备份也是在这个硬盘上，那相当于招了灭顶之灾。

所以巴菲特有个关于投资的名言，就是“不要把鸡蛋放在一个篮子里”。对于系统而言也是如此。厂商的机子不可能永远保证永远不坏，我们也无法保证黑客不会来对我们的系统搞基，最为关键的是，我们自己无法保证自己的程序不会出bug。所以问题无法避免，错误也不可避免。我们只能鸡蛋分散到不同的篮子里，来减轻一锅端的风险。这就是为什么需要分布式系统的原因。

使用分布式系统的另外一个理由是可扩展性。毕竟任何主机（哪怕是小型机、超级计算机）都会有性能的极限。而分布式系统可以通过不断扩张主机的数量以实现横向水平性能的扩展。大家也都了解到 Google 的服务器主机，大多是淘汰的二线机子拼凑的吧。

* 分布式系统会面临哪里挑战

毫无疑问，分布式系统对于集中式系统而言，在实现上会更加复杂。分布式系统将会是更难理解、设计、构建 和管理的，同时意味着应用程序的根源问题更难发现。

设计分布式系统时，经常需要考虑如下的挑战：

- 异构性：分布式系统由于基于不同的网络、操作系统、计算机硬件和编程语言来构造，必须要考虑一种通用的网络通信协议来屏蔽异构系统之间的差异。一般交由中间件来处理这些差异。
- 缺乏全球时钟：在程序需要协作时，它们通过交换消息来协调它们的动作。紧密的协调经常依赖于对程序动作发生时间的共识，但是，实际上网络上计算机同步时钟的准确性受到极大的限制，即没有一个正确时间的全局概念。这是通过网络发送消息作为唯一的通信方式这一事实带来的直接结果。
- 一致性：数据被分散或者复制到不同的机器上，如何保证各台主机之间的数据的一致性将成为一个难点。
- 故障的独立性：任何计算机都有可能故障，且各种故障不尽相同。他们之间出现故障的时机也是相互独立的。一般分布式系统要设计成被允许出现部分故障而不影响整个系统的正常使用。
- 并发：分布式系统的目的，是为了更好的共享资源。那么系统中的每个资源都必须被设计成在并发环境中是安全的。
- 透明性：分布式系统中任何组件的故障、或者主机的升级、迁移对于用户来说都是透明的，不可见的。
- 开放性：分布式系统由不同的程序员来编写不同的组件，组件最终要集成成为一个系统，那么组件所发布的接口必须遵守一定的规范且能够被互相理解。
- 可扩展性：系统要设计成随着业务量的增加，相应的系统也必须要能扩展来提供对应的服务。

* 如何来设计分布式

设计分布式系统的本质就是“如何合理将一个系统拆分成多个子系统部署到不同机器上”。所以首要考虑的问题是如何合理的将系统进行拆分。由于拆分后的各个子系统不可能孤立的存在，必然是通过网络进行连接交互，所以它们之间如何通信变得尤为重要。当然在通信过程要识别“敌我”，防止信息在传递过程中被拦截和窜改，这就涉及到安全问题了。分布式系统要适应不断增长的业务需求，那么就需要考虑其扩展性。分布式系统还必须要保证可靠性和数据的一致性。

概况起来，在设计分布式系统时，应考虑以下几个问题：

- 系统如何拆分为子系统？
- 如何规划子系统间的通信？
- 通信过程中的安全如何考虑？
- 如何让子系统可以扩展？
- 子系统的可靠性如何保证？
- 数据的一致性是如何实现的？

实际上，上面的每一个问题都不是简单的问题。还好，我们要感谢开源，让这个时代的技术可以共享，让实现复杂系统的成本越来越低。比如，我们在设计通信时，我们可以采用面向消息的中间件，比如Apache ActiveMQ、RabbitMQ、Apache RocketMQ、Apache Kafka等，也有类似与 Google Protocol Buffer、Thrift等 RPC框架。在设计分布式计算时，我们分布式计算可以采用 MapReduce、Apache Hadoop、Apache Spark 等。在大数据和分布式存储方面，我们可以选择 Apache HBase、Apache Cassandra、Memcached、Redis、MongoDB等。在分布式监控方面，常用的技术包括Nagios、Zabbix、Consul、ZooKeeper等。

# 一致性算法 Paxos & raft & zab

## Raxos 算法（少数服从多数）

解决的问题：在一个可能发生异常的分布式系统中如何就某个值达成一致，让整个集群的节点对某个值的变更达成一致
任何一个节点都可以提出要修改某个数据的提案,是否通过这个提案取决于这个集群中是否有超过半数的节点同意（所以节点数总是单数）—— 版本标记。虽然一致性，但是只能对一个操作进行操作啊？？
当一个Server接收到比当前版本号小的提案时，则拒绝。当收到比当前大的版本号的提案时，则锁定资源，进行修改，返回OK.   也就是说收到超过一半的最大版本的提案才算成功。

* 核心思想：

在抢占式访问权的基础上引入多个acceptor，也就是说当一个版本号更大的提案可以剥夺版本号已经获取的锁。
后者认同前者的原则：

- 在肯定旧epoch 无法生成确定性取值时，新的 epoch 会提交自己的valu
- 一旦 旧epoch形成确定性取值，新的 epoch肯定可以获取到此取值，并且会认同此取值，不会被破坏。

## ZAB 协议 ( Zookeeper Atomic  Broadcast)

原子广播协议：保证了发给各副本的消息顺序相同

ZAB 是在 Paxos 算法基础上进行扩展而来的。Zookeeper 使用单一主进程 Leader用于处理客户端所有事务请求，采用 ZAB 协议将服务器状态以事务形式广播到所有 Follower 上，由于事务间可能存在着依赖关系，ZAB协议保证 Leader 广播的变更序列被顺序的处理，一个状态被处理那么它所依赖的状态也已经提前被处理

核心思想：保证任意时刻只有一个节点是Leader，所有更新事务由Leader发起去更新所有副本 Follower，更新时用的是两段提交协议，只要多数节点 prepare 成功，就通知他们commit。各个follower 要按当初 leader 让他们 prepare 的顺序来 apply 事务。

## Raft 算法

演示：http://thesecretlivesofdata.com/raft/

Raft 算法也是一种少数服从多数的算法，在任何时候一个服务器可以扮演以下角色之一：

- Leader：负责 Client 交互 和 log 复制，同一时刻系统中最多存在一个
- Follower：被动响应请求 RPC，从不主动发起请求 RPC
- Candidate : 由Follower 向Leader转换的中间状态

在选举Leader的过程中，是有时间限制的，raft 将时间分为一个个 Term，可以认为是“逻辑时间”：

- 每个 Term中至多存在1个 Leader
- 某些 Term由于不止一个得到的票数一样，就会选举失败，不存在Leader。则会出现 Split Vote  ，再由候选者发出邀票
- 每个 Server 本地维护 currentTerm


## 分布式系统原理

### 一、数据分布式方式

解决问题： 如何将大量数据分散在不同机器上面。
1.1 哈希方式

    哈希方式 ： 按照数据的某个特征计算哈希值，并将哈希值跟机器建立映射关系，从而不同的哈希值数据分布式到不同的机器上面。
    优点：方法简单，元数据在管理也简单。
    缺点：
        （1）扩展性不高，集群需要升级所有的数据几乎都需要被迁移和分布。
        （2）一旦数据严重不均匀的问题，容易出现“数据倾斜”（data skew）问题。
    优化：
        (1）针对问题1，不是简单的哈希值与机器做除法取模映射，而是使用专门的元数据管理服务器单独管理。

1.2 按数据范围分布

    将数据按特征值的值域范围划分为不同的区间，这样集群中的服务器处理不同区间的数据。
    工程中为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得每个区间中服务的数据量尽量一样多。一般需要专门的元数据管理服务器记录所有数据的分布情况（大规模的集群需要多台元信息服务器）。
    优点：
        （1）可以灵活的根据数据量的具体情况拆分原有数据区间，拆分后的数据区间可以迁移到其他集群。
        （2）集群扩容的时候只需要将部分数据迁移即可。
    缺点：
        （1） 需要维护复杂的元信息数据。
        （2）当集群规模扩大的时候，元数据服务器较为容易成为瓶颈，需要引入复杂的多元数据服务器机制解决这个问题。

1.3 按数据量分布

    按数据量分布：不关心数据的特征值，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小的块（chunk），不同的块分布到不同服务器。数据块的管理同样需要元数据管理服务器。
    优点：
        （1）因为是按块划分，所有没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。
        （2）重新负载均衡的时候，只需要通过迁移数据块即可。
        （3）扩容的时候，只需要通过迁移部分的数据块即可。
    缺点：
        （1） 复杂的数据块管理，需要专门的元数据管理服务器。
        （2） 当集群规模扩大的时候，元数据服务器较为容易成为瓶颈，需要引入复杂的多元数据服务器机制解决这个问题。

1.4 一致性哈希

    一致性哈希： 用一个哈希函数计算数据或者数据特征的哈希值，令哈希函数的输出值域为一个封闭的环。节点会分布在这个环上处理一个范围内的哈希值。
    优点：
        （1）可以任意动态添加、删除节点，每次添加、删除节点仅影响一致性哈希环上相邻的节点。
        （2）节点在一致性哈希环上的位置作为元数据管理，数据的大小只跟集群规模相关，数据量比按数据范围分布和按数据量分布的元信息量小很多。
    缺点：
        （1）随机分布节点的方式使得很难均匀的分布哈希值域，当异常发生的时候，该节点的压力全部转移到相邻的一个节点，当加入一个新节点的时候只能相邻的节点分摊压力。

改良算法：引入虚节点的概念。

    系统初始化的时创建很多虚拟节点，虚拟节点的个数远大于未来集群中机器的个数，将虚拟节点均匀分布到一致性哈希值域环上。为每个节点分配若干的虚拟节点（不是连续的）。操作时先找到虚拟节点，在映射到真实的节点。
    优点：
        （1）某个节点异常，该节点的多个虚拟节点不可用，从而多个相邻的真实节点负载节点失效的压力。
        （2）加入节点的时候，可以分配多个虚节点，从而使得新节点可以负载原有的多个节点压力。

### 二、数据副本

一：以机器为单位的数据副本

    以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。
    优点：非常简单。
    缺点：
        （1） 恢复数据的效率不高，可扩展性也不高。
        如 三台副本，如果一台机器磁盘损坏，加入新机器需要从其他两台拷贝数据。全盘拷贝很消耗资源。为了不影响服务质量，实践往往提高两种方式：
            a: 将一台副本下线，专门做数据源拷贝（这样的话，实际副本就只有一台了，数据存在巨大的安全隐患）,在有些要求副本数量的分布式协议中根本不可用。
            b: 以较低的资源使用限速方法从两个副本上面拷贝数据，但是速度很慢。
        （2） 不利于提高系统扩展性

二: 以数据段为单位的数据副本

    将数据拆分为合理的的数据，以数据段为单位作为副本，实践中常常使得每个数据段的大小尽量相等且控制在一定的大小以内。数据段有很多称谓如：segment、fragment、chunk、partition等等。
    缺点： 需要管理数据段的分布信息，完全按照数据建立副本管理元数据的开销较大，同时副本维护的难度也相应增大。（折中方式将数据段分为数据段分组，按照数据段组进行副本管理）
    优点：数据恢复很快，因为一台机器数据丢失，数据段副本可能分布在集群的全部机器中，可以从整个集群拷贝数据。

### 三、副本协议



    副本控制协议：指副本按特定的协议流程控制副本数据的读写行为，使得副本满足一定的可用性和一致性要求的分布式协议。分为两类：中心化副本控制协议和去中心化副本控制协议

一：中心化副本控制协议

    由一个中心节点协调副本数据的更新、维护副本之间的一致性。
    优点：协议相对简单，所有副本相关的控制交由中心节点完成。
    缺点：系统的可用性依赖于中心化节点，当中心节点异常或者中心节点通信中断时，系统将失去某些服务。

1： primary-secondary协议

    primary-secondary又称为primary-backup，在协议中副本被分为两类，有且只有一个primary副本，其他的全部是secondary副本。
    primary-secondary解决四大类问题：
        数据更新流程
        数据读取方式
        Primary副本的确定和切换
        数据同步
    a：数据更新流程
        （1）：数据更新都有primary节点协调完成。
        （2）：外部节点将更新操作发送给primary节点。
        （3）：primary节点进行并发控制即确定并发更新操作的先后顺序。
        （4）：primary节点将更新操作发送给secondary节点。
        （5）：primary根据secondary节点的完成情况决定更新是否成功并将结果返回给外部节点。
    b： 数据读取
        如果只需要最终一致性，则读取任何副本都可以满足需求。如果需要会话一致性，则可以为副本设置版本号，每次更新后递增版本号，则用户读取副本时验证版本号，从而保证用户读到的数据都在会话范围内递增。想要最终一致性比较困难，由一下几种思路参考.
            (１)：只读primary节点。
            (２)：由primary控制节点secondary的可用性，更新成功为可用，更新不成功为不可用。
            (３): 基于Quorum机制。
    c: primary副本的确定与切换
        通常在primary-secondary类型的分布式系统中，哪个副本是Primary是属于元信息，由专门的元数据服务器维护。执行更新操作前先查询元数据管理服务器获取primary信息，再进行操作。
        切换副本的难度在于两个方面：
            (1)：如何确定节点的状态以及发现原primary节点异常时一个较为复杂的问题。

            基于Lease机制、基于Quorum机制。

            (2)：在原primary以及死机，如何确定一个secondary副本使得该副本上面的数据与原primary一致成为新的问题。
    由于分布式系统中可靠的发现节点异常需要一点的探测时间的，探测时间通常在10秒级别。 primary-secondary类型的分布式系统的最大缺点就是primary切换带来的一定时间的停服务时间。
    d：数据同步
        通常的不一致情况:
            (1)： 由于网络分化等异常，secondary上面的数据落后primary上的数据。
            (2)：在某些协议下面可能有脏数据，需要被丢弃。
            (3)：secondary是新增的副本，完全没有数据，需要从其他副本拷贝数据。
        解决方法
            对于（1），常用的方法是回放primary上面的操作日志（通常是redo日志），从而追上primary的更新进度。
            对于（2），较好的方法就是在设计分布式协议不产生脏数据。也可以基于undo日志的方法删除脏数据。
            对于（3），直接拷贝primary数据，或者使用快照。

二：去中心化副本控制协议

    去中心化协议没有因为中心节点异常带来的停服务问题，缺点是协议过程通常比较复杂。

### 四、Lease机制 

问题背景：

    含有中心服务器的节点，管理着元数据，其他节点需要读取和修改这些数据。每次操作都需要访问中心服务器节点，那么节点很容易成为瓶颈。为此设计一种元数据cache，要求cache数据始终与中心服务器上面的数据一致。
    原理：
    中心服务器在向各个节点发送数据时同时向节点颁发一个lease。每个lease具有一个有效期，lease上面的有效期通常是一个明确的时间点。（这边先假设中心服务器和各节点时钟是同步的），在lease的有效期内，中心服务器保证不修改对应数据的值。节点收到数据和lease后，将数据加入本地Cache，一旦对应的lease超时节点删除本地Cache。中心服务器修改数据是，先阻塞所有新的读请求，并等待之前为该数据发送的所有的lease超时过期，然后修改数据的值。
    优化：上面中心服务器进入修改数据流程后，会阻塞读请求，优化方式是处理读请求但是不发布新的lease。
    Lease机制依赖于有效期，这就要求颁布者和接收者的时钟是同步的。对于时钟不同步的问题，实践中的方法是将颁发者的有效期设置得比接收者的略大，只需要过时钟误差就可以避免对lease的有效性影响。
    基于Lease机制确定节点状态
    情景： 存在A,B,C,Q四个节点，ABC为副本，Q需要判断ABC状态。
    基于Lease机制的过程如下：
        ABC周期性的向Q发送心跳，Q收到心跳之后发送一个lease，表示Q确定了节点A、B、C的状态，允许在lease时期内正常工作。节点Q可以给primary节点一个特殊的lease，表示节点可以作为primary工作，一旦节点Q希望切换新的primary，则只需要等前一个primary的lease过期，则就可以安全的颁发新的lease给新的primary，而不会出现“双主”问题。
        实际系统中，若用一个中心节点发送lease风险很大，一旦该中心节点崩溃或者网络异常则所有节点都没有lease，从而造成系统的高度不可用。实际系统使用多个中心节点互为副本成为一个小的集群，如chubby和zookeeper。
        工程中，常选择的lease时长是10秒级别，这是一个经过验证的经验值，实践可以中可以作为参考并综合选择合适的时长。

### 五、Quorum 机制 


    WARO是一种最简单的副本控制规则，即在更新时写副本所有的副本，只有在所有副本更新成功，才认为更新成功，从而保证所有副本的一致性，这样读取时可以读取任何一个副本上面的数据。
    需要一种机制当某次更新操作Wi一旦所有副本都成功，全局都能知道这个消息，此后读取操作指定读取数据版本为Vi的数据。
    在工程实践中，这种机制往往比较难实现或者效率较低，通常方法就是将版本号信息存放在某个或某租元数据服务器上。记录更新成功的版本号Vi的操作将成为关键操作，容易成为瓶颈。因为更新需要全部副本成功才能成功，所有当副本不可用的情况，更新服务就不可用。读服务而言系统可以容忍N-1个副本异常。

Quorum：

    Quorum是一种简单的副本管理机制。WARO牺牲了更新服务的可用性，最大程度的增强了读服务的可用性。Quorum在WARO之上做了限制而来。
    约定：
        更新操作是一系列顺序的过程，通过其他机制确定更新操作的顺序(如primary-secondary架构中的primary)，每次更新记录操作记为Wi，i为更新操作单调递增的序号，每个Wi执行成功后副本数据都变化，称为不同数据版本，记为Vi。
    在Quorum机制下面，当某次更新操作Wi一旦在所有的N个副本都成功，则认为更新操作为“成功提交的更新操作”，对于的数据为“成功提交的数据”。令R>N-W,由于更新在W个副本上面是成功的，所有最多读取R个副本则一定能读到Wi更新后的数据Vi。当W=N，R=1，就得到WARO。
    仅仅依赖Quorum机制是无法保证强一致性的，因为Quorum机制无法确定最新已经提交的版本号，除非将最新已经提交的版本号作为元数据由特定的元数据服务器或者元数据机器管理。

如何读取最新成功提交的数据：

    令N=5，W=3，R=3，某个时刻该副本最大版本号为（v2，v2，v2，v1，v1），读取三个副本不能判定v2一定是成功提交的版本。（v2，v2，v2，v1，v1）（v2，v1，v1，v1，v1）。对于强一致性系统，应该始终读取返回最新的成功提交的数据，在Quorum机制下面条件需要进一步限制。
        1：限制提交的更新操作必须严格递增，即只有在前一个更新操作成功提交之后才可以提交后一个更新操作，从而保证提交的数据版本是连续增加的。
        2: 读取R个副本，对于R个副本中版本号最高的数据，
            2.1 若已经存在W个，则该数据是最新的成功提交的数据 （怎么确定是已经有W个了？）
            2.2 若存在个数少于W个，为X个，继续读取其他副本，直到读取到W个该版本的副本，则认为是最新提交的数据。

基于Quorum机制选择Primary

    在primary-secondary协议中primary复杂进行更新的同步的操作，限制加入Quorum机制，即primary成功更新W个副本后向用户返回成功。读取数据按照一致性的要求而不同。如果需要强一致性，读取primary本身就返回即可。如何需要会话一致性，根据之前读取到的版本号在各个副本进行选择性读取；如果只需要弱一致性则选择任意副本读取即可。
    当primary异常需要选择一个新的primary，通常选择primary的工作需要某一个中心节点完成，在引入Quorum机制后，常用的primary选择方式与读取数据方式类似，即中心节点读取R个副本，选择R个副本中版本号最高的副本作为新的primary。新的primary至少与W个副本完成数据同步之后作为新的primary提高读写服务。

### 六、日志技术

日志技术是死机恢复的主要技术之一。
Redo Log：
Redo Log更新流程：

    1：讲更新操作的结果以追加的方式写入磁盘的日志文件
    2：按更新操作修改内存中的数据
    3：返回更新成功
    Redo Log宕机恢复流程：
    1：从头读取日志文件中的每次更新操作的结果，用这些结果修改内存中的数据。

Check Point：

    宕机恢复流量的缺点是需回放所有的redo日志，效率较低。为了解决这个问题引入了check point。check point将内存中的数据以某种易于重新加载的数据组织方式完整的dump到磁盘，从而减少宕机恢复时回放的日志数据。（日志是操作日志，check point是内存中的数据）
    基于check point的宕机恢复流程:
        1：将dump到磁盘的数据加载到内存。
        2：从后向前扫描日志文件，需要最后一个“End Check Point”日志。
        3：从最后一个“End Check Point”日志向前找到最近的一个“Begin Check Point”日志，并回放之后的所有更新日志操作。

No Undo/No Redo Log:

    No Undo/No Redo Log 被称为“0/1”目录
    情景： 数据维护在磁盘，某批更新由若干更新操作组成，这些更新操作需要原子生效，要么同时生效，要么都不生效。
    0/1目录技术中两个目录结构，称为目录0和目录1，另有一个结构称为主目录，负责记录当前活动正在使用的目录（活动目录）。
    0/1目录数据更新流程：
        1: 将活动目录完整拷贝到非活动目录。
        2：对于每个更新操作，新建一个日志项记录操作后的值，并将非活动目录中将相应的数据的位置修改为新建目录的日志项位置。
        3：原子性修改主记录，反转主记录中的值，使得非活动目录失效。

### 七、2PC 3PC

2PC

    两阶段提交协议(Two-phase Commit，2PC),经常用来实现分布式事务，系统中一般包括两类节点：一类为协调者(coordinator)，通常只有一个；另一类为事务参与者(worker)，一般包含几个。
    过程如下：
        阶段1：请求阶段(Prepare Phase)
        协调者通知事务参与者准备提交或者取消事务，然后进入表决过程。在表决过程中，参与者将告诉协调者自己的决策：同意或者取消。
        阶段2：提交阶段(Commit Phase)
        在提交阶段，协调者将基于第一个阶段的投票结果进行决策；提交或者取消。当且仅当所有的参与者同意提交事务，协调者才通知所有的参与者提交事务，或者协调者通知所有的参与取消事务。参与者在接收到协调者发来的消息后将执行相应的操作。

2PC可能面临的两种故障：

    1：事务参与者故障
        解决方法：给每个事务设置一个超时时间，如果某个事务参与者一直不响应，到达超时时间后整个事务失败。
    2：协调者发送故障
        协调者需要将事务相关的信息记录到操作日志并同步到备用协调者，假如协调者发生故障，备用协调者可以接替它完成后续的工作。如果没有备用协调者，协调者发送永久性故障，事务参与者无法完成事务而一直等待下去。
    两阶段提交协议是阻塞协议，执行过程中需要锁住其他更新，且不能容错，大多数分布式存储系统都采用敬而远之的做法，放弃对分布式事务的支持。

3PC

    与两阶段提交不同的是，三阶段提交有两个改动点。
        1、引入超时机制。同时在协调者和参与者中都引入超时机制。
        2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。
    执行过程
        阶段一：请求阶段。
            事务请求阶段，协调者询问参与者并获取反馈。
        阶段二：准备阶段。
            若参与者无法正常反馈，则结束事务。否则，发送执行事务操作指令，参与者执行具体事务并记录Undo和Redo信息，并反馈结果。
        阶段三：提交阶段。
            协调者正常工作且通信正常时，发送事务提交请求或者取消请求。否则，参与者会在等待超时之后自行提案事务。
    两阶段提交与三阶段提交的区别：
        没有任何事情是完美的。特别是在分布式的情况下。事实上，分布式在某个程度上其实是人类社会发展的一个极佳写真。因为人类社会中个体的可靠性显然比分布式系统节点的可靠性要低很多。
    三阶段提交也不完美。但是它比两阶段好。
        两阶段的问题可以这样分解：
            1，协调者出错，参与者也出错；
            2，协调者出错，参与者不出错；
            3，协调者不出错，参与者出错；
            4，协调者不出错，参与者也不出错。
        显然第4种不是问题。所以实际上只有3个问题。而问题2可以通过简单地NEW一个新的协调者来解决。问题3的错则显然正是两阶段提交协议的解决目标，所以也没有问题。
        有问题的只有协调者出错，参与者也出错的问题1。这种情况可以被进一步分为参与者有没有收到提交的消息。如果参与者没有收到提交的消息，那么显然将不会（或没有—从系统恢复的角度）发生任何真正的提交行为；而如果有任何参与者收到了提交的消息，那么就很可能发生或已经发生了真正的提交行为。这个“可能”，为系统引入了不确定因素。系统没有办法解决这样的问题，唯一的办法便是引入超时机制。否则除了事务没有办法终结以外，部分参与者节点还有可能永不释放其所持有的全部数据锁。
        超时机制的引入意味着将两阶段的第二阶段再度分开成两个阶段：不确定阶段与确定阶段。超时以前是不确定操作阶段，超时以后是确定操作阶段。因为在超时发生以前，系统处于不确定阶段，但是超时发生以后，系统则转入确定阶段。超时事件本身，则是系统进行状态转换的信号。但是因为真正引起超时的错只会在协调者与参与者同时出错（对于不出错但超时的情况，视为出错。即超时本身就是一种错—如果超时不“是”错，那么超时机制在这里就不可能工作—这其实就是超时机制的逻辑根本所在。超时是一种错，所以超时可以被用来表示错。如果用一种不是错的信号来表示错，那要区分真正的错就会很困难了）的情况下才会发生，在其它所有的情况下并不会发生，所以必须对这些情况进行相同的状态划分：准备好与提交状态。这些名词并不是很合乎它要表示的语义，但两个状态足够表达所有的情况才是最重要的事情。

### 八、基于 MVCC 的分布式事务

MVCC简介：

    MVCC（Multi-version Cocurrent Control，多版本并发控制技术），即多个不同版本的数据实现并发控制技术，基本思想是为每次事务生成一个新版本的数据，在读数据时选择不同版本的数据既可以实现对事务结果的完整性读取。
    基础数据版本为V1，同时产生两个事务：事务A和事务B,都需要对各自的数据进行修改，事务A生成数据版本V2，基于数据版本V2发起事务C，事务C继续提交生成鼠标版本V3，最后事务B提交，此时事务B的结果需要与事务C的结果合并。如果数据没有冲突就合并，或者事务B提交失败。
    事务在基于基础数据版本做本地修改时，为了不影响真正的数据，通常有两种做法：（1）将基础数据拷贝出来再修改,比如SVN;（2）每个事务只记录更新操作，而不记录完整的数据，读取数据时再将更新操作应用到用基础版本的数据从而计算结果，类似SVN的增量提交。

分布式MVCC

    分布式MVCC重点不在于并发控制，在于实现分布式事务。
    情景：
        假设在一个分布式系统中，更新操作以事务进行，每个事务对若干不同节点的同步更新操作。更新事务比较具有原子性。
    基于MVCC的分布式方法为：
        为每个事务分配一个递增的事务编号，这个编号表示数据的版本号。当事务在各个节点上面执行的时候，各个节点只需要记录更新操作及事务编号，当事务在各个节点完成的时候，在全局元信息中记录本次事务的编号。在读取数据时，先读取元信息中已经成功的最大事务编号，再于各个节点上读取数据，只读更新操作编号小于等于最后最大已成功提交事务编号的操作，并将这些操作应用到基础数据形成读取结果。
